{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tBot_project.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "f8ae86d605b4439da49dbebd2a9a3cbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_5c6f608ee5554fe195aa056e086dee48",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_d140a1d892a5431cb0992524da802c4c",
              "IPY_MODEL_e7508801f6e04ae19f70df42978e90bb"
            ]
          }
        },
        "5c6f608ee5554fe195aa056e086dee48": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d140a1d892a5431cb0992524da802c4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_98c514b644744ea49ddef4242ba0251f",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 574673361,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 574673361,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cb45d40c3cce4e309cac5b48fa4535a5"
          }
        },
        "e7508801f6e04ae19f70df42978e90bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7fce41358cab41869422ee9a5487adc9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 548M/548M [00:36&lt;00:00, 15.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0b6d76f69ed34c829caed9e7667f55ae"
          }
        },
        "98c514b644744ea49ddef4242ba0251f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cb45d40c3cce4e309cac5b48fa4535a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7fce41358cab41869422ee9a5487adc9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0b6d76f69ed34c829caed9e7667f55ae": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "80GcqjD2naXl"
      },
      "source": [
        "#Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k6RJGW6QVYtJ",
        "outputId": "ec2b2f36-5dcb-4203-f28a-44584a4c101c"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r31jOICFXK8B"
      },
      "source": [
        "#==================================================================\r\n",
        "# to NST_net\r\n",
        "#==================================================================\r\n",
        "from PIL import Image\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import numpy as np\r\n",
        "import torch\r\n",
        "import torch.optim as optim\r\n",
        "from torchvision import transforms\r\n",
        "from torchvision.utils import save_image\r\n",
        "from torchvision.models import vgg19\r\n",
        "import requests\r\n",
        "from IPython.display import clear_output\r\n",
        "%matplotlib inline"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bHULWQPA6BmX"
      },
      "source": [
        "TOKEN = '1663109481:AAEAu9pghz8DNRcUdT64s5O4tGmaEMq0H0s'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WkVlnACvwL2O"
      },
      "source": [
        "#NST_net"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YwH_IV3IX3sV",
        "outputId": "829ebfbc-f660-426b-8f54-10ab8913184e"
      },
      "source": [
        "torch.backends.cudnn.benchmark = True\r\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\r\n",
        "print(device)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xA9B6uNJXy9N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103,
          "referenced_widgets": [
            "f8ae86d605b4439da49dbebd2a9a3cbb",
            "5c6f608ee5554fe195aa056e086dee48",
            "d140a1d892a5431cb0992524da802c4c",
            "e7508801f6e04ae19f70df42978e90bb",
            "98c514b644744ea49ddef4242ba0251f",
            "cb45d40c3cce4e309cac5b48fa4535a5",
            "7fce41358cab41869422ee9a5487adc9",
            "0b6d76f69ed34c829caed9e7667f55ae"
          ]
        },
        "outputId": "fc91b9d5-8181-4cca-f0e5-4081881e2207"
      },
      "source": [
        "backbone = vgg19(pretrained=True).features.to(device).eval().requires_grad_(False)\r\n",
        "\r\n",
        "for (i, layer) in enumerate(backbone):\r\n",
        "  if isinstance(layer, torch.nn.MaxPool2d):\r\n",
        "    backbone[i] = torch.nn.AvgPool2d(kernel_size=2, stride=2, padding=0)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/vgg19-dcbb9e9d.pth\" to /root/.cache/torch/hub/checkpoints/vgg19-dcbb9e9d.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8ae86d605b4439da49dbebd2a9a3cbb",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, max=574673361.0), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fA96vdpVwQmm"
      },
      "source": [
        "def load_image(name_img, max_size=256, shape=None):\r\n",
        "    \r\n",
        "    image = Image.open(name_img).convert('RGB')\r\n",
        "    \r\n",
        "    if max(image.size) > max_size:\r\n",
        "        size = max_size\r\n",
        "    else:\r\n",
        "        size = max(image.size)\r\n",
        "    \r\n",
        "    if shape is not None:\r\n",
        "        size = shape\r\n",
        "        \r\n",
        "    in_transform = transforms.Compose([\r\n",
        "                        transforms.Resize(size),\r\n",
        "                        transforms.ToTensor(),\r\n",
        "                        transforms.Normalize((0.485, 0.456, 0.406), \r\n",
        "                                             (0.229, 0.224, 0.225))])\r\n",
        "\r\n",
        "    # discard the alpha channel (that's the :3) and add the batch dimension\r\n",
        "    image = in_transform(image)[:3,:,:].unsqueeze(0)\r\n",
        "    \r\n",
        "    return image"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P30xN1xskf3O"
      },
      "source": [
        "# convert tesnor to image, denormalize\r\n",
        "def im_convert(tensor):\r\n",
        "  # tensor.detach() creates a tensor that shares storage with tensor that does not require grad. \r\n",
        "  # It detaches the output from the computational graph. So no gradient will be backpropagated along this variable.\r\n",
        "  image = tensor.to(\"cpu\").clone().detach()\r\n",
        "  image = image.numpy().squeeze()\r\n",
        "  image = image.transpose(1, 2, 0) # to -> H, W, C\r\n",
        "  image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406))\r\n",
        "  # Image needs to be clipped between 0 and 1 or it looks like noise when displayed\r\n",
        "  image = image.clip(0, 1)\r\n",
        "  return image"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ti5QatQw8yWY"
      },
      "source": [
        "##Net building and Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lmElEmJz8hrb"
      },
      "source": [
        "def gram_matrix(tensor):\r\n",
        "    batch_size, channels, height, width = tensor.size()\r\n",
        "    \r\n",
        "    tensor = tensor.view(batch_size * channels, height * width)\r\n",
        "    gram = tensor @ tensor.t()\r\n",
        "    \r\n",
        "    return gram\r\n",
        "\r\n",
        "def get_features(image, model, layers=None):\r\n",
        "    if layers is None:\r\n",
        "        layers = {'0': 'conv1_1',\r\n",
        "                  '5': 'conv2_1', \r\n",
        "                  '10': 'conv3_1', \r\n",
        "                  '19': 'conv4_1',\r\n",
        "                  '21': 'conv4_2',  # content layer\r\n",
        "                  '28': 'conv5_1'}\r\n",
        "    features = {}\r\n",
        "    x = image\r\n",
        "    \r\n",
        "    for name, layer in model._modules.items():\r\n",
        "        x = layer(x)\r\n",
        "        if name in layers:\r\n",
        "            features[layers[name]] = x\r\n",
        "            \r\n",
        "    return features"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TsmnTw7KTYac"
      },
      "source": [
        "def NST_fit(content, style, backbone, num_epochs=3000):\r\n",
        "\r\n",
        "  # get_content features\r\n",
        "  content_features = get_features(content, backbone)\r\n",
        "  # grad for content\r\n",
        "  target = content.clone().requires_grad_(True).to(device)\r\n",
        "\r\n",
        "  # get_style features\r\n",
        "  style_features = get_features(style, backbone)\r\n",
        "  # gram matrix for all in style_features\r\n",
        "  style_grams = {layer: gram_matrix(style_features[layer]) for layer in style_features}\r\n",
        "\r\n",
        "  # learn pams/optim\r\n",
        "  optimizer = optim.AdamW([target], lr=3e-3)\r\n",
        "\r\n",
        "  style_weights = {'conv1_1': 0.1,\r\n",
        "                  'conv2_1': 0.2,\r\n",
        "                  'conv3_1': 0.2,\r\n",
        "                  'conv4_1': 0.5,\r\n",
        "                  'conv5_1': 0.75}\r\n",
        "  content_weight = 1 \r\n",
        "  style_weight = 1e5\r\n",
        "  \r\n",
        "  # train loop\r\n",
        "  for ii in range(1, num_epochs+1):\r\n",
        "\r\n",
        "      # refresh target_features\r\n",
        "      target_features = get_features(target, backbone)\r\n",
        "      # refresh_content_loss\r\n",
        "      content_loss = torch.mean((target_features['conv4_2'] - content_features['conv4_2'])**2)\r\n",
        "      # init style_loss\r\n",
        "      style_loss = 0\r\n",
        "\r\n",
        "      for layer in style_weights:\r\n",
        "          # get the \"target\" style representation for the layer\r\n",
        "          target_feature = target_features[layer]\r\n",
        "\r\n",
        "          _, d, h, w = target_feature.shape\r\n",
        "          \r\n",
        "          # get the \"target\" gram matrix\r\n",
        "          target_gram = gram_matrix(target_feature)\r\n",
        "          # get the \"style\" style representation\r\n",
        "          style_gram = style_grams[layer]\r\n",
        "\r\n",
        "          # the style loss for one layer, weighted appropriately\r\n",
        "          layer_style_loss = style_weights[layer] * torch.mean((target_gram - style_gram)**2)\r\n",
        "          # add to the style loss\r\n",
        "          style_loss += layer_style_loss / (d * h * w)\r\n",
        "\r\n",
        "      total_loss = content_weight * content_loss + style_weight * style_loss \r\n",
        "      \r\n",
        "      # update your target image\r\n",
        "      optimizer.zero_grad()\r\n",
        "      total_loss.backward()\r\n",
        "      optimizer.step()\r\n",
        "\r\n",
        "  return target"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF6Y3ogIl5mx"
      },
      "source": [
        "#Telebot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qF-wA-k9l7p6",
        "outputId": "599041f0-162f-4716-9d88-b0063a8cf370"
      },
      "source": [
        "!pip3 install PyTelegramBotAPI"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting PyTelegramBotAPI\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3d/44/dda2c6a43306b27eb23c327f780eb48479dfeda15f764f47004ebe60205e/pyTelegramBotAPI-3.7.6.tar.gz (81kB)\n",
            "\r\u001b[K     |████                            | 10kB 23.7MB/s eta 0:00:01\r\u001b[K     |████████                        | 20kB 19.7MB/s eta 0:00:01\r\u001b[K     |████████████                    | 30kB 15.6MB/s eta 0:00:01\r\u001b[K     |████████████████                | 40kB 13.9MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 51kB 10.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████▏       | 61kB 9.6MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 71kB 10.9MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 81kB 5.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from PyTelegramBotAPI) (2.23.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->PyTelegramBotAPI) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->PyTelegramBotAPI) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->PyTelegramBotAPI) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->PyTelegramBotAPI) (2020.12.5)\n",
            "Building wheels for collected packages: PyTelegramBotAPI\n",
            "  Building wheel for PyTelegramBotAPI (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for PyTelegramBotAPI: filename=pyTelegramBotAPI-3.7.6-cp36-none-any.whl size=59261 sha256=2e87dd52a327ac4dfa98f00b088101a3d4ef0b096c3768a2366762ac71c61852\n",
            "  Stored in directory: /root/.cache/pip/wheels/f6/66/6e/d42e8fcb446d2683b5afe0b23318f4bb58896bad26549c47b9\n",
            "Successfully built PyTelegramBotAPI\n",
            "Installing collected packages: PyTelegramBotAPI\n",
            "Successfully installed PyTelegramBotAPI-3.7.6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3iIKLC5Rmxcn"
      },
      "source": [
        "import telebot\r\n",
        "import os"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HW018AQymCEz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "710a660d-e580-46a2-9ed1-e1fc9115fda1"
      },
      "source": [
        "bot = telebot.TeleBot(TOKEN)\r\n",
        "\r\n",
        "@bot.message_handler(commands=['start'])\r\n",
        "def start_message(message):\r\n",
        "    bot.send_message(message.chat.id, 'Привет, вы написали NST_bot(у)!\\\r\n",
        "                                      \\nЯ могу перенести стиль одной картинки на другую.\\\r\n",
        "                                      \\nЕсли Вам нужна помощь, то напишите --> /help')\r\n",
        "@bot.message_handler(commands=['help'])\r\n",
        "def help_message(message):\r\n",
        "    bot.send_message(message.chat.id, \r\n",
        "                'В данном боте есть несколько методов переноса стиля:\\\r\n",
        "                \\n(1) <ваш стиль на вашу картинку>.\\\r\n",
        "                \\n Для этого Вы присылаете сразу две картинки: первая фото-контекст, вторая фото-стиль ПОСЛЕ пишите --> /My\\\r\n",
        "                \\n(2) <стиль вангога/волны на вашу картинку>\\\r\n",
        "                \\n Для этого Вы присылаете фото-контекст ПОСЛЕ пишите --> /VanGog или /Wave')\r\n",
        "\r\n",
        "@bot.message_handler(content_types=['photo'])  \r\n",
        "def get_images(message): \r\n",
        "  if message.media_group_id:\r\n",
        "\r\n",
        "    file_info = bot.get_file(message.photo[len(message.photo) - 1].file_id)\r\n",
        "\r\n",
        "    downloaded_file = bot.download_file(file_info.file_path)\r\n",
        "\r\n",
        "    src = '/content/files/' + file_info.file_path\r\n",
        "    with open(src, 'wb') as new_file:\r\n",
        "        new_file.write(downloaded_file)\r\n",
        "    \r\n",
        "    bot.reply_to(message, \"Принял\")      \r\n",
        "\r\n",
        "    path = '/content/files/photos' # Путь к вашей папке\r\n",
        "\r\n",
        "    # Получим список имен всего содержимого папки\r\n",
        "    # и превратим их в абсолютные пути\r\n",
        "    dir_list = [os.path.join(path, x) for x in os.listdir(path)]\r\n",
        "\r\n",
        "    if dir_list:\r\n",
        "        # Создадим список из путей к файлам и дат их создания.\r\n",
        "        date_list = [[x, os.path.getctime(x)] for x in dir_list]\r\n",
        "\r\n",
        "        # Отсортируем список по дате создания в обратном порядке\r\n",
        "        sort_date_list = sorted(date_list, key=lambda x: x[1], reverse=True)\r\n",
        "\r\n",
        "        # Выведем первый элемент списка. Он и будет самым последним по дате\r\n",
        "        print(str(sort_date_list[0][0]))\r\n",
        "        print(str(sort_date_list[1][0]))\r\n",
        "\r\n",
        "        style_im = Image.open(str(sort_date_list[0][0])).convert('RGB')\r\n",
        "        style_im.save('/content/style.jpg')\r\n",
        "        context_im = Image.open(str(sort_date_list[1][0])).convert('RGB')\r\n",
        "        context_im.save('/content/context.jpg')\r\n",
        "\r\n",
        "  else:\r\n",
        "    file_info = bot.get_file(message.photo[-1].file_id)\r\n",
        "    downloaded_file = bot.download_file(file_info.file_path)\r\n",
        "    with open('/content/context.jpg', \"wb\") as new_file:\r\n",
        "      new_file.write(downloaded_file)\r\n",
        "    bot.send_message(message.chat.id, 'Ваше фото-контекст получено!')\r\n",
        "\r\n",
        "@bot.message_handler(commands=['VanGog'])\r\n",
        "def handle_NST_VanGog(message):\r\n",
        "  bot.send_message(message.chat.id, 'Стиль Ван Гога на ваше фото скоро будет перенесен')\r\n",
        "  # content im.\r\n",
        "  content = load_image('/content/context.jpg').to(device)\r\n",
        "  # styles im., resize style to content\r\n",
        "  style = load_image('/content/drive/MyDrive/NST_VanGog.jpg',\r\n",
        "                        shape=content.shape[-2:]).to(device)\r\n",
        "  target = NST_fit(content, style, backbone)\r\n",
        "  im = Image.fromarray((im_convert(target) * 255).astype(np.uint8))\r\n",
        "  bot.send_photo(message.chat.id, im)\r\n",
        "\r\n",
        "@bot.message_handler(commands=['Wave'])\r\n",
        "def handle_NST_Wave(message):\r\n",
        "  bot.send_message(message.chat.id, 'Стиль Волны Хокусая на ваше фото скоро будет перенесен')\r\n",
        "  # content im.\r\n",
        "  content = load_image('/content/context.jpg').to(device)\r\n",
        "  # styles im., resize style to content\r\n",
        "  style = load_image('/content/drive/MyDrive/NST_Wave.jpg',\r\n",
        "                        shape=content.shape[-2:]).to(device)\r\n",
        "  target = NST_fit(content, style, backbone)\r\n",
        "  im = Image.fromarray((im_convert(target) * 255).astype(np.uint8))\r\n",
        "  bot.send_photo(message.chat.id, im)\r\n",
        "\r\n",
        "@bot.message_handler(commands=['My'])\r\n",
        "def handle_NST_My(message):\r\n",
        "  bot.send_message(message.chat.id, 'Прошу Вас немного подождать')\r\n",
        "  # content im.\r\n",
        "  content = load_image('/content/context.jpg').to(device)\r\n",
        "  # styles im., resize style to content\r\n",
        "  style = load_image('/content/style.jpg',\r\n",
        "                        shape=content.shape[-2:]).to(device)\r\n",
        "  target = NST_fit(content, style, backbone)\r\n",
        "  im = Image.fromarray((im_convert(target) * 255).astype(np.uint8))\r\n",
        "  bot.send_photo(message.chat.id, im)\r\n",
        "\r\n",
        "bot.polling()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/files/photos/file_26.jpg\n",
            "/content/files/photos/file_25.jpg\n",
            "/content/files/photos/file_26.jpg\n",
            "/content/files/photos/file_25.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gEpJG2bhJ5DN"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}